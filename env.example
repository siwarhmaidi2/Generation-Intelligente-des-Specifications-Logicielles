# Provider : "ollama" (local via Ollama, RECOMMANDÉ ✅), "local" (transformers), "mistral" (API cloud), ou "openai" (API compatible)
LLM_PROVIDER=ollama

# Pour Ollama avec Mistral quantifié (RECOMMANDÉ) - Pas besoin d'API key !
LLM_API_BASE=http://localhost:11434
LLM_MODEL=mistral:7b-instruct-q4_K_M
LLM_API_KEY=

# Timeout pour les requêtes (en secondes) - optionnel
# Par défaut : 1800s (30 min) pour Ollama, 60s pour les autres
# Pour les textes très longs, tu peux augmenter : LLM_TIMEOUT=3600 (60 min)
# LLM_TIMEOUT=1800

# -------- MODE RAPIDE (répondre vite) --------
# ANALYST_MODE=fast
# LLM_MAX_TOKENS_FAST=900
# OLLAMA_NUM_PREDICT=900
# OLLAMA_NUM_CTX=2048
# Modèle plus rapide (moins bon) : LLM_MODEL=phi3:mini

# Pour le mode local (transformers) - décommente si besoin :
# LLM_PROVIDER=local
# LLM_MODEL=microsoft/Phi-3-mini-4k-instruct
# LLM_API_KEY=
# LLM_API_BASE=

# Pour Mistral AI (cloud, nécessite API key) :
# LLM_PROVIDER=mistral
# LLM_API_KEY=votre_cle_api_mistral_ici
# LLM_MODEL=mistral-small

