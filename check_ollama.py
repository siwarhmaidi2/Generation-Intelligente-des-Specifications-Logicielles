"""
Script de v√©rification pour diagnostiquer les probl√®mes avec Ollama.
"""
import httpx
import os
from dotenv import load_dotenv

load_dotenv()

def check_ollama():
    """V√©rifie que Ollama est accessible et que le mod√®le est disponible."""
    api_base = os.getenv("LLM_API_BASE", "http://localhost:11434")
    model = os.getenv("LLM_MODEL", "mistral:7b-instruct-q4_K_M")
    
    print("üîç V√©rification de la configuration Ollama...\n")
    print(f"   API Base: {api_base}")
    print(f"   Mod√®le: {model}\n")
    
    # V√©rifier la connexion √† Ollama
    print("1Ô∏è‚É£ V√©rification de la connexion √† Ollama...")
    try:
        url = f"{api_base}/api/tags"
        with httpx.Client(timeout=5.0) as client:
            resp = client.get(url)
            resp.raise_for_status()
            data = resp.json()
        print("   ‚úÖ Ollama est accessible\n")
    except httpx.ConnectError:
        print("   ‚ùå Impossible de se connecter √† Ollama")
        print("   üí° Solutions :")
        print("      - V√©rifie qu'Ollama est lanc√© : `ollama list`")
        print("      - Si Ollama n'est pas lanc√©, d√©marre-le depuis le menu D√©marrer")
        return False
    except Exception as e:
        print(f"   ‚ùå Erreur : {e}")
        return False
    
    # V√©rifier que le mod√®le est disponible
    print("2Ô∏è‚É£ V√©rification des mod√®les disponibles...")
    models = [m.get("name", "") for m in data.get("models", [])]
    
    if not models:
        print("   ‚ö†Ô∏è  Aucun mod√®le trouv√© dans Ollama")
        print(f"   üí° T√©l√©charge le mod√®le : `ollama pull {model}`")
        return False
    
    print(f"   üì¶ Mod√®les disponibles ({len(models)}):")
    for m in models:
        marker = "‚úÖ" if model in m else "  "
        print(f"      {marker} {m}")
    
    # V√©rifier si le mod√®le demand√© est disponible
    model_found = any(model in m for m in models)
    if not model_found:
        print(f"\n   ‚ùå Mod√®le '{model}' non trouv√©")
        print(f"   üí° T√©l√©charge le mod√®le : `ollama pull {model}`")
        return False
    
    print(f"\n   ‚úÖ Mod√®le '{model}' est disponible\n")
    
    # Test de g√©n√©ration rapide
    print("3Ô∏è‚É£ Test de g√©n√©ration rapide...")
    try:
        url = f"{api_base}/api/generate"
        payload = {
            "model": model,
            "prompt": "Bonjour",
            "stream": False,
        }
        with httpx.Client(timeout=30.0) as client:
            resp = client.post(url, json=payload)
            resp.raise_for_status()
        print("   ‚úÖ G√©n√©ration test r√©ussie\n")
        return True
    except httpx.ReadTimeout:
        print("   ‚ö†Ô∏è  Timeout lors du test (peut √™tre normal si le mod√®le est en train de se charger)")
        print("   üí° R√©essaie dans quelques secondes")
        return True  # On consid√®re que c'est OK, juste lent
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur lors du test : {e}")
        print("   üí° Cela peut √™tre normal, r√©essaie le script principal")
        return True

if __name__ == "__main__":
    success = check_ollama()
    if success:
        print("‚úÖ Tout semble correct ! Tu peux lancer `python analyze_example.py`")
    else:
        print("\n‚ùå Des probl√®mes ont √©t√© d√©tect√©s. Corrige-les avant de continuer.")
