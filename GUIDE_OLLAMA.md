# Guide rapide - Ollama avec Mistral quantifi√©

## üöÄ Installation en 3 √©tapes

### 1. Installer Ollama

T√©l√©charge et installe depuis : https://ollama.com/download

Ollama d√©marre automatiquement apr√®s l'installation.

### 2. T√©l√©charger Mistral 7B quantifi√©

```powershell
ollama pull mistral:7b-instruct-q4_K_M
```

**Temps estim√©** : 10-20 minutes (mod√®le ~4GB)

### 3. Configurer le projet

```powershell
# Cr√©er l'environnement virtuel
python -m venv .venv
.venv\Scripts\Activate.ps1

# Installer les d√©pendances
pip install -r requirements.txt

# Cr√©er le fichier .env (d√©j√† configur√© pour Ollama)
Copy-Item env.example .env
```

**C'est tout !** Le fichier `.env` est d√©j√† configur√© avec :
```env
LLM_PROVIDER=ollama
LLM_API_BASE=http://localhost:11434
LLM_MODEL=mistral:7b-instruct-q4_K_M
LLM_API_KEY=
```

## ‚úÖ V√©rification

```powershell
# V√©rifier qu'Ollama fonctionne
ollama list

# Tu devrais voir : mistral:7b-instruct-q4_K_M

# Tester l'agent
python analyze_example.py
```

## üìä Avantages de Mistral quantifi√©

- **RAM n√©cessaire** : ~5GB (au lieu de 16GB pour le mod√®le complet)
- **Qualit√©** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (presque identique au mod√®le complet)
- **Taille** : ~4GB (au lieu de 13GB)
- **Vitesse** : Rapide, surtout avec GPU NVIDIA

## üîß D√©pannage

### Ollama ne d√©marre pas
- Relance l'application Ollama depuis le menu D√©marrer
- V√©rifie qu'aucun autre processus n'utilise le port 11434

### Mod√®le non trouv√©
```powershell
ollama pull mistral:7b-instruct-q4_K_M
```

### Erreur "Connection refused"
- V√©rifie qu'Ollama est lanc√© : `ollama list`
- V√©rifie que `LLM_API_BASE=http://localhost:11434` dans `.env`

## üí° Autres mod√®les disponibles

Si tu veux essayer d'autres mod√®les :

```powershell
# Phi-3 Mini (tr√®s l√©ger, ~2GB RAM)
ollama pull phi3:mini

# Mistral complet (non quantifi√©, ~16GB RAM)
ollama pull mistral:7b-instruct
```

Puis change `LLM_MODEL` dans `.env` en cons√©quence.
